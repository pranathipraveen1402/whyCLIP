<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <title>CLIP Feature Extraction vs Traditional Feature Ext</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet">
</head>
<body>
    <div class="container mt-5">
        <h1 class="text-center">CLIP Feature Extraction vs Traditional Feature Extraction</h1>
        <div class="row mt-4">
            <div class="col-md-6 text-center">
                <img src="TFDesc.png" class="img-fluid rounded" alt="Feature Extraction Illustration">
            </div>
            <div class="col-md-6">
                <p>
                    CLIP (Contrastive Language-Image Pretraining) extracts features by aligning images and text in a shared embedding space,
                    enabling zero-shot learning. Traditional feature extraction methods, such as SIFT, HOG, and deep CNNs,
                    rely on predefined heuristics or supervised training on specific datasets.
                </p>
                <ul>
                    <li><strong>CLIP:</strong> Learns from vast amounts of image-text pairs.</li>
                    <li><strong>Traditional Methods:</strong> Require task-specific training.</li>
                    <li><strong>Flexibility:</strong> CLIP enables zero-shot tasks without retraining.</li>
                </ul>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
</body>
</html>